\documentclass[12pt]{article}

\setlength{\parskip}{1em}

\usepackage[T1]{fontenc}
\usepackage[a4paper, margin=0.7in]{geometry}
\usepackage{amsfonts}
\usepackage{mathabx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}


\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  pdftitle={PC40 Hands-on: UPC}
}

% \definecolor{gray}{rgb}{0.5,0.5,0.5}
% \definecolor{purple}{rgb}{0.58,0,0.82}
\definecolor{bgColor}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{C}{
    backgroundcolor=\color{bgColor},
    commentstyle=\color{gray},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{purple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=C
}

\newcommand{\us}[0]{${\mu}s$}

\title{PC40 Hands-on: UPC}
\author{Adrien Burgun}
\date{Automne 2021}
% \graphicspath{{report/}}

\begin{document}
\maketitle

\begin{abstract}
  For this hands-on assignement, I wanted to measure the performance of the different versions of the code and compare them to measure speedups.
  For this reason, my code diverges slightly from the template that was given to us, as I needed to insert a timing method and avoid refactoring the code.

  You will find in this report listings of the different versions of the code, alongside experimental measurements and commentaries.

  The source code itself, this report's source code and instructions on how to build and run the code yourself can be found on this project's git repository: \url{https://github.com/adri326/pc40-upc/}.
\end{abstract}

\section{Simplified 1D Laplace solver}
\label{sec:laplace}

\subsection{C implementation}

The C implementation of the laplace solver has been slightly modified to time the main loop.
It is single-threaded but the implementation allows for compiler SIMD optimizations.
Settings common to every version (vector size, epsilon, max number of iterations) have been placed in a file called \texttt{settings.h}.

This code has been compiled with \texttt{gcc v4.9.0} and run on the \texttt{mesoshared} server, yielding the following results:

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    \texttt{-O0} & 450\us/iter & $\pm$ 10\us/iter \\
    \texttt{-O3} & 140\us/iter & $\pm$ 10\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the C implementation of the 1D Laplace solver (Listing~\ref{lst:laplace2})}
  \label{tab:laplace2}
\end{table}

\lstinputlisting[style=C, label={lst:laplace2}, caption={
  C implementation of the 1D Laplace solver
}]{laplace/ex2.c}

\subsection{Porting the C code to UPC}

The base C implementation was designed to be able to quickly port it to UPC.
A new \texttt{if} statement had to be inserted in the \texttt{for} loop within \texttt{iteration()}.
Additionally, several lines had to only be executed by the thread 0, so additional conditionals were added when needed.

Finally, the \texttt{x}, \texttt{xnew} and \texttt{b} arrays were made shared and \texttt{upc\_barrier} statements were added at the end of \texttt{init}, \texttt{iteration} and \texttt{copy\_array}:
this is to prevent threads from beginning processing the next iteration when the \texttt{x} and \texttt{xnew} arrays aren't ready, and to prevent the thread 0 from stopping early and printing the wrong timing.

This code was compiled with \texttt{upcc v2.22.0 + gcc v4.2.4} and run on the \texttt{mesoshared} server, yielding the following results:

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 515.2\us/iter & $\pm$ 3.9\us/iter \\
    3 & 536.2\us/iter & $\pm$ 2.4\us/iter \\
    4 & 312.4\us/iter & $\pm$ 2.0\us/iter \\
    8 & 204.0\us/iter & $\pm$ 2.0\us/iter \\
    16 & 166.2\us/iter & $\pm$ 2.5\us/iter \\
    32 & 150.4\us/iter & $\pm$ 3.2\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the first UPC implementation of the 1D Laplace solver (Listing~\ref{lst:laplace3})}
  \label{tab:laplace3}
\end{table}

When compiled and run with 3 threads, the code runs noticeably slower. For curiosity, I ran the code with 24 and 31 threads and obtained a similar slowdown:

\begin{center}
  Threads = 24, \quad Time = 252.6\us/iter $\pm$ 3.7\us/iter \enspace (expected $\approx 160$ \us/iter) \\
  Threads = 31, \quad Time = 317.6\us/iter $\pm$ 4.4\us/iter \enspace (expected $\approx 150$ \us/iter)
\end{center}

\lstinputlisting[style=C, label={lst:laplace3}, caption={
  First UPC implementation of the 1D Laplace solver
}]{laplace/ex3.upc}

\subsection{Optimizing the inner for loop}

The first step in optimizing our UPC implementation of the 1D Laplace equation solver is to replace the \texttt{for (...) if (...)} with a single, more efficient \texttt{for} loop.

To achieve this, only the \texttt{iteration()} function had to be changed.
This transformation is shown in Figure~\ref{fig:laplace34}

This change greatly increases the speed of the program:

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 482.0\us/iter & $\pm$ 3.2\us/iter \\
    4 & 265.2\us/iter & $\pm$ 2.3\us/iter \\
    8 & 128.8\us/iter & $\pm$ 1.6\us/iter \\
    16 & 74.6\us/iter & $\pm$ 3.1\us/iter \\
    32 & 46.0\us/iter & $\pm$ 2.5\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the second UPC implementation of the 1D Laplace solver}
  \label{tab:laplace4}
\end{table}

\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// ex3.upc:
void iteration() {
  for (size_t i = 1; i < LEN - 1; i++) {
      if (i % THREADS == MYTHREAD) {
          x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
      }
  }

  if (MYTHREAD == 0) x_new[0] = x[0];
  if (MYTHREAD == (LEN - 1) % THREADS) x_new[LEN - 1] = x[LEN - 1];

  upc_barrier;
}
    \end{lstlisting}
    \caption{Previous \texttt{iteration()} function from Listing~\ref{lst:laplace3}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// ex4.upc:
void iteration() {
    size_t i = MYTHREAD;
    if (MYTHREAD == 0) {
        i += THREADS;
        x_new[0] = x[0];
    }

    for (; i < LEN - 1; i += THREADS) {
        x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
    }

    if (MYTHREAD == THREADS - 1) x_new[LEN - 1] = x[LEN - 1];

    upc_barrier;
}
    \end{lstlisting}
    \caption{New \texttt{iteration()} function without the inner \texttt{if}}
  \end{subfigure}
  \caption{Modification in \texttt{iteration()} to remove the reduce the number of loop iterations.}
  \label{fig:laplace34}
\end{figure}

% \lstinputlisting[style=C, label={lst:laplace4}, caption={
%   Second UPC implementation of the 1D Laplace solver
% }]{laplace/ex4.upc}

\subsection{Optimizing the array blocking factor}

Our next optimization is to increase the blocking factor $B$ for the $x$, $x_{new}$ and $b$ arrays.
Each operation on the item of index $j$ accesses $b[j]$, $x[j-1]$, $x[j+1]$ and $x_{new}[j]$.

With the default block factor of $1$, accesses to both $x[j-1]$ and $x[j+1]$ will always be outside of the current thread's affinity.
With a greater block factor, these outside accesses can be reduced to only $2/B$ accesses on average.

Two parts of the code had to be changed: the array declarations (Figure~\ref{fig:laplace45a}) and the loop in \texttt{iteration()} and \texttt{copy\_array()} (Figure~\ref{fig:laplace45b}).

The effects of this change is dependent on $B$: similar to the results found in Table~\ref{tab:laplace3}, the code runs fastest when $B = 2^n$.
Whichever value is chosen, however, the program did not run faster than \texttt{ex4.upc} (Table~\ref{tab:laplace4}), but rather ran much slower (\href{https://github.com/adri326/pc40-upc/tree/main/laplace/ex5.upc}{the source code of this version can be found here}):

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 959.2\us/iter & $\pm$ 3.9\us/iter \\
    4 & 863.9\us/iter & $\pm$ 16.1\us/iter \\
    8 & 691.6\us/iter & $\pm$ 6.1\us/iter \\
    16 & 581.5\us/iter & $\pm$ 4.1\us/iter \\
    32 & 536.9\us/iter & $\pm$ 4.0\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the third UPC implementation of the 1D Laplace solver, $B = 32$}
  \label{tab:laplace5}
\end{table}

When ran with $B = 1$, $THREADS = 32$, the code takes 318\us/iter $\pm$ 7.8\us/iter.
I do not know what causes this slowdown, but it seems like \texttt{upc\_forall} could contribute to it.

When limiting the modification of \texttt{ex4.upc} to only replace the \texttt{for} loop with a \texttt{upc\_forall} loop, the example becomes $83\%$ slower (206\us/iter $\rightarrow$ 376\us/iter, 8 threads, AMD Ryzen 2700X, \texttt{upcc v2.28.2 + gcc v9.3.0}).

\subsubsection{Note on the update/copy loop}

So far, the program has been running many iterations of the "main" loop, which calls \texttt{iteration()} and \texttt{copy\_array()}.
A \texttt{upc\_barrier;} call is necessary at the end of each of those two operations, as \texttt{copy\_array()} modifies $x$ and depends on the processed value of $x_{new}$, while \texttt{iteration()} modifies $x_{new}$ and depends on the copied value in $x$.

\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// ex4.upc:
shared double x_new[LEN];
shared double x[LEN];
shared double b[LEN];
    \end{lstlisting}
    \caption{Previous array declarations}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// settings.h:
#define BLOCKSIZE 32

// ex5.upc:
shared[BLOCKSIZE] double x_new[LEN];
shared[BLOCKSIZE] double x[LEN];
shared[BLOCKSIZE] double b[LEN];
    \end{lstlisting}
    \caption{New array declarations with blocking factor}
  \end{subfigure}
  \caption{Modification of the array declarations to increase the blocking factor $B$.}
  \label{fig:laplace45a}
\end{figure}

\begin{lstlisting}[style=C, numbers=none, caption={New \texttt{iteration()} and \texttt{copy\_array()} implementations}, label={fig:laplace45b}]
// ex5.upc:
void iteration() {
    upc_forall (size_t i = 1; i < LEN - 1; i++; &x_new[i]) {
        x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
    }

    if (MYTHREAD == 0) x_new[0] = x[0];
    if (MYTHREAD == THREADS - 1) x_new[LEN - 1] = x[LEN - 1];

    upc_barrier;
}

void copy_array() {
    upc_forall (size_t i = 0; i < LEN; i++; &x_new[i]) {
        x[i] = x_new[i];
    }
    upc_barrier;
}
\end{lstlisting}

\newpage

\subsection{Detecting convergence}

One of our last modifications is to measure $\delta_{max} = \max_{i\in \ldbrack 0; n-1 \rdbrack}{\bigl\vert x[i]-x_{new}[i] \bigr\vert}$ (named \texttt{diffmax} in the source code) and to stop once $\delta_{max} \leq \varepsilon$.

Because of the shared nature of $x$ and $x_{new}$, we need to compute a partial $\delta_{max}^t$ and compute $\delta_{max} = \max_{t \in [0; T[}(\delta_{max}^t)$, with:

$$\delta_{max}^t = \max_{i\in \ldbrack 0; n-1 \rdbrack, \; \textrm{\tiny affinity}(x_{new}[i]) = t}{\Bigl\vert x[i]-x_{new}[i] \Bigr\vert}$$

In the implementation, we use one of the collective vector functions provided by UPC.
This function contains implicit \texttt{upc\_barrier;} instructions, so we do not need to add them ourselves.

The new timings are:

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 1400.4\us/iter & $\pm$ 6.0\us/iter \\
    4 & 1092.8\us/iter & $\pm$ 17.0\us/iter \\
    8 & 803.4\us/iter & $\pm$ 7.7\us/iter \\
    16 & 647.8\us/iter & $\pm$ 18.9\us/iter \\
    32 & 572.0\us/iter & $\pm$ 4.6\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the fourth UPC implementation of the 1D Laplace solver, $B = 32$, \texttt{MAX\_ITER = 1000}}
  \label{tab:laplace6}
\end{table}

\begin{lstlisting}[style=C, caption={Excerpt from ex6.upc: new \texttt{handle\_diff} function and calculation of \texttt{diffmax}}, label={lst:laplace6}]
// ex6.upc (partial):
shared double diff[THREADS];
shared double diffmax;

void handle_diff(double d) {
    if (d < 0.0) d = -d;
    if (diff[MYTHREAD] < d) diff[MYTHREAD] = d;
}

int main() {
    // ...
    while (true) {
        iteration();

        upc_barrier;

        if (MYTHREAD == 0) {
            for (size_t n = 0; n < THREADS; n++) {
                if (diffmax < diff[n]) diffmax = diff[n];
            }
        }

        upc_barrier;

        if (diffmax <= EPSILON) break;
        if (++iter > MAX_ITER) break;

        copy_array(); // upc_barrier; at the end of copy_array();
    }
    // ...
}

void iteration() {
    upc_forall (size_t i = 1; i < LEN - 1; i++; &x_new[i]) {
        x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
        handle_diff(x_new[i] - x[i]);
    }
}
\end{lstlisting}

\subsubsection{Using reduction operations}

We can further optimize this code by using reduction operations from \texttt{upc\_collective.h}.
The optimized variant is available at \texttt{\href{https://github.com/adri326/pc40-upc/tree/main/laplace/ex5.upc}{laplace/optimized.upc}}.

By removing the calls to \texttt{upc\_forall}, we can reduce the time taken by a further $35\% (\pm 5\%pt)$.

% Unfortunately, no real performance benefit was observed, even with 32 threads.

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 1216.0\us/iter & $\pm$ 27.1\us/iter \\
    4 & 778.0\us/iter & $\pm$ 15.1\us/iter \\
    8 & 415.8\us/iter & $\pm$ 10\us/iter \\
    16 & 225.8\us/iter & $\pm$ 4.8\us/iter \\
    32 & 130.0\us/iter & $\pm$ 3.1\us/iter \\
    64 & 98.2\us/iter & $\pm$ 2.7\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the fully optimized UPC implementation of the 1D Laplace solver, $B = 32$, \texttt{MAX\_ITER = 1000}}
  \label{tab:laplace6opt}
\end{table}

\subsection{Conclusion (1D Laplace Solver)}

The first implementation in UPC of the 1D Laplace solver algorithm and its subsequent first optimization (flattening the \texttt{for (...) if (...)}) gave very promising results for the improvement of the speed of the program, bringing the timings down from 140\us/iteration to only 46\us/iteration.

Unfortunately, the next optimization attempt, which introduces a blocking factor to the primary arrays and used \texttt{upc\_forall}, brought the speeds down and made the UPC implementation much slower than the C implementation.

The addition of $\delta_{max}$, as a way to stop the algorithm once a sufficiently accurate solution is found, inevitably added another overhead to the program.
With my best efforts, I could only bring its performance to somewhere between those of the first implementation and its first optimization.

Nonetheless, the resulting program runs slightly faster than the original C implementation, given enough threads).

\section{2D Heat conduction}

For this second algorithm, we will try to improve on the knowledge gathered from the first algorithm and implement a simple, 2D heat conduction simulation.
As with Section~\ref{sec:laplace}, we begin with a simple C implementation of the algorithm, which will work as our performance base value.
The provided template came bundled with a performance measurement method, but I swapped it out with \texttt{clock()} in the subsequent UPC implementations for consistency with the last section's code.

\subsection{First C implementation}

Following are the performance results for the first C implementation, ran on \texttt{mesoshared} with \texttt{gcc v4.9.0} and on an \texttt{AMD Ryzen 2700X} with \texttt{clang v12.0.1}:

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    \texttt{-O0} & 89.1\us/iter & $\pm$ 0.3\us/iter \\
    \texttt{-O3} & 15.0\us/iter & $\pm$ 0.1\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_c.c} (mesoshared, Listing~\ref{lst:heatc})}
  \label{tab:heatc}
\end{table}

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    \texttt{-O0} & 80.9\us/iter & $\pm$ 2.6\us/iter \\
    \texttt{-O3} & 17.9\us/iter & $\pm$ 2.2\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_c.c} (Ryzen 2700X, Listing~\ref{lst:heatc})}
  \label{tab:heatcr}
\end{table}

\lstinputlisting[style=C, label={lst:heatc}, caption={
  C implementation of the 2D Heat simulation
}]{2d_heat/heat_c.c}

\subsection{Porting the C code to UPC}

Porting the template C version to UPC is straightforward.
The performance, however, takes a hit, as the default work sharing causes a lot of remote accesses to memory, despite the blocking factor that was put in place.
We obtain the following measurements on \texttt{mesoshared}:



\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 259.9\us/iter & $\pm$ 1.9\us/iter \\
    4 & 193.6\us/iter & $\pm$ 1.5\us/iter \\
    8 & 164.1\us/iter & $\pm$ 1.2\us/iter \\
    16 & 150.6\us/iter & $\pm$ 1.4\us/iter \\
    32 & 151.7\us/iter & $\pm$ 1.7\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_1.upc}}
  \label{tab:heat1}
\end{table}

\subsection{Optimizing the array accesses}

One simple optimization is to avoid copying the destination array (\texttt{new\_grid}) into the source array (\texttt{grid}).

To do this, we store two shared pointers that will point on either array, and we swap them at the end of each iteration, simulating a copy of \texttt{grid} into \texttt{new\_grid}, with a $\mathcal{O}(1)$ time complexity instead of $\mathcal{O}(n)$.
Doing so effectively halves the amount of synchronization that needs to happen for each loop, which can be observed in the timing results:

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 134.0\us/iter & $\pm$ 1.4\us/iter \\
    4 & 103.9\us/iter & $\pm$ 4.1\us/iter \\
    8 & 87.4\us/iter & $\pm$ 1.0\us/iter \\
    16 & 81.4\us/iter & $\pm$ 1.2\us/iter \\
    32 & 82.9\us/iter & $\pm$ 1.5\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_3.upc}}
  \label{tab:heat3}
\end{table}

\subsection{Array privatization}

As observed in the later examples of the 1D Laplace equation solver (Table~\ref{tab:laplace5}), \texttt{upc\_forall} has a significant performance cost over a regular \texttt{for} loop.
We also know that local shared memory accesses have a considerable cost when the shared pointer math is done without dedicated hardware support. % cite Olivier Serres here -- too lazy now

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 13.5\us/iter & $\pm$ 1.4\us/iter \\
    4 & 12.6\us/iter & $\pm$ 1.2\us/iter \\
    8 & 13.3\us/iter & $\pm$ 1.0\us/iter \\
    16 & 15.0\us/iter & $\pm$ 1.2\us/iter \\
    32 & 21.5\us/iter & $\pm$ 1.1\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_4.upc}}
  \label{tab:heat4}
\end{table}

\end{document}
