\documentclass[12pt]{article}

\setlength{\parskip}{1em}

\usepackage[T1]{fontenc}
\usepackage[a4paper, margin=0.7in]{geometry}
\usepackage{amsfonts}
\usepackage{mathabx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{makecell}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  pdftitle={PC40 Hands-on: UPC}
}

% \definecolor{gray}{rgb}{0.5,0.5,0.5}
% \definecolor{purple}{rgb}{0.58,0,0.82}
\definecolor{bgColor}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{C}{
    backgroundcolor=\color{bgColor},
    commentstyle=\color{gray},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{purple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=C
}

\newcommand{\us}[0]{${\mu}s$}

\title{PC40 Hands-on: UPC}
\author{Adrien Burgun}
\date{Automne 2021}
% \graphicspath{{report/}}

\begin{document}
\maketitle

\begin{abstract}
  For this hands-on assignement, I wanted to measure the performance of the different versions of the code and compare them to measure speedups.
  For this reason, my code diverges slightly from the template that was given to us, as I needed to insert a timing method and avoid refactoring the code.

  You will find in this report listings of the different versions of the code, alongside experimental measurements and commentaries.

  The source code itself, this report's source code and instructions on how to build and run the code yourself can be found on this project's git repository: \url{https://github.com/adri326/pc40-upc/}.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Simplified 1D Laplace solver}
\label{sec:laplace}

\subsection{C implementation}

The C implementation of the laplace solver has been slightly modified to time the main loop.
It is single-threaded but the implementation allows for compiler SIMD optimizations.
Settings common to every version (vector size, epsilon, max number of iterations) have been placed in a file called \texttt{settings.h}.

This code has been compiled with \texttt{gcc v4.9.0} and run on the \texttt{mesoshared} server, yielding the following results:

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    \texttt{-O0} & 450\us/iter & $\pm$ 10\us/iter \\
    \texttt{-O3} & 140\us/iter & $\pm$ 10\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the C implementation of the 1D Laplace solver (Listing~\ref{lst:laplace2})}
  \label{tab:laplace2}
\end{table}

\lstinputlisting[style=C, label={lst:laplace2}, caption={
  C implementation of the 1D Laplace solver
}]{laplace/ex2.c}

\subsection{Porting the C code to UPC}

The base C implementation was designed to be able to quickly port it to UPC.
A new \texttt{if} statement had to be inserted in the \texttt{for} loop within \texttt{iteration()}.
Additionally, several lines had to only be executed by the thread 0, so additional conditionals were added when needed.

Finally, the \texttt{x}, \texttt{xnew} and \texttt{b} arrays were made shared and \texttt{upc\_barrier} statements were added at the end of \texttt{init}, \texttt{iteration} and \texttt{copy\_array}:
this is to prevent threads from beginning processing the next iteration when the \texttt{x} and \texttt{xnew} arrays aren't ready, and to prevent the thread 0 from stopping early and printing the wrong timing.

This code was compiled with \texttt{upcc v2.22.0 + gcc v4.2.4} and run on the \texttt{mesoshared} server, yielding the following results:

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 515.2\us/iter & $\pm$ 3.9\us/iter \\
    3 & 536.2\us/iter & $\pm$ 2.4\us/iter \\
    4 & 312.4\us/iter & $\pm$ 2.0\us/iter \\
    8 & 204.0\us/iter & $\pm$ 2.0\us/iter \\
    16 & 166.2\us/iter & $\pm$ 2.5\us/iter \\
    32 & 150.4\us/iter & $\pm$ 3.2\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the first UPC implementation of the 1D Laplace solver (Listing~\ref{lst:laplace3})}
  \label{tab:laplace3}
\end{table}

When compiled and run with 3 threads, the code runs noticeably slower. For curiosity, I ran the code with 24 and 31 threads and obtained a similar slowdown:

\begin{center}
  Threads = 24, \quad Time = 252.6\us/iter $\pm$ 3.7\us/iter \enspace (expected $\approx 160$ \us/iter) \\
  Threads = 31, \quad Time = 317.6\us/iter $\pm$ 4.4\us/iter \enspace (expected $\approx 150$ \us/iter)
\end{center}

\lstinputlisting[style=C, label={lst:laplace3}, caption={
  First UPC implementation of the 1D Laplace solver
}]{laplace/ex3.upc}

\subsection{Optimizing the inner for loop}

The first step in optimizing the UPC implementation of the 1D Laplace equation solver is to replace the \texttt{for (...) if (...)} with a single, more efficient \texttt{for} loop.

To achieve this, only the \texttt{iteration()} function had to be changed.
This transformation is shown in Figure~\ref{fig:laplace34}

This change greatly increases the speed of the program:

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 482.0\us/iter & $\pm$ 3.2\us/iter \\
    4 & 265.2\us/iter & $\pm$ 2.3\us/iter \\
    8 & 128.8\us/iter & $\pm$ 1.6\us/iter \\
    16 & 74.6\us/iter & $\pm$ 3.1\us/iter \\
    32 & 46.0\us/iter & $\pm$ 2.5\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the second UPC implementation of the 1D Laplace solver}
  \label{tab:laplace4}
\end{table}

\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// ex3.upc:
void iteration() {
  for (size_t i = 1; i < LEN - 1; i++) {
      if (i % THREADS == MYTHREAD) {
          x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
      }
  }

  if (MYTHREAD == 0) x_new[0] = x[0];
  if (MYTHREAD == (LEN - 1) % THREADS) x_new[LEN - 1] = x[LEN - 1];

  upc_barrier;
}
    \end{lstlisting}
    \caption{Previous \texttt{iteration()} function from Listing~\ref{lst:laplace3}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// ex4.upc:
void iteration() {
    size_t i = MYTHREAD;
    if (MYTHREAD == 0) {
        i += THREADS;
        x_new[0] = x[0];
    }

    for (; i < LEN - 1; i += THREADS) {
        x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
    }

    if (MYTHREAD == THREADS - 1) x_new[LEN - 1] = x[LEN - 1];

    upc_barrier;
}
    \end{lstlisting}
    \caption{New \texttt{iteration()} function without the inner \texttt{if}}
  \end{subfigure}
  \caption{Modification in \texttt{iteration()} to remove the reduce the number of loop iterations.}
  \label{fig:laplace34}
\end{figure}

% \lstinputlisting[style=C, label={lst:laplace4}, caption={
%   Second UPC implementation of the 1D Laplace solver
% }]{laplace/ex4.upc}

\subsection{Optimizing the array blocking factor}
\label{sec:laplace5}

Our next optimization is to increase the blocking factor $B$ for the $x$, $x_{new}$ and $b$ arrays.
Each operation on the item of index $j$ accesses $b[j]$, $x[j-1]$, $x[j+1]$ and $x_{new}[j]$.

With the default block factor of $1$, accesses to both $x[j-1]$ and $x[j+1]$ will always be outside of the current thread's affinity.
With a greater block factor, these outside accesses can be reduced to only $2/B$ accesses on average.

Two parts of the code had to be changed: the array declarations (Figure~\ref{fig:laplace45a}) and the loop in \texttt{iteration()} and \texttt{copy\_array()} (Figure~\ref{fig:laplace45b}).

The effects of this change is dependent on $B$: similar to the results found in Table~\ref{tab:laplace3}, the code runs fastest when $B = 2^n$.
Whichever value is chosen, however, the program did not run faster than \texttt{ex4.upc} (Table~\ref{tab:laplace4}), but rather ran much slower (\href{https://github.com/adri326/pc40-upc/tree/main/laplace/ex5.upc}{the source code of this version can be found here}):

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 959.2\us/iter & $\pm$ 3.9\us/iter \\
    4 & 863.9\us/iter & $\pm$ 16.1\us/iter \\
    8 & 691.6\us/iter & $\pm$ 6.1\us/iter \\
    16 & 581.5\us/iter & $\pm$ 4.1\us/iter \\
    32 & 536.9\us/iter & $\pm$ 4.0\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the third UPC implementation of the 1D Laplace solver, $B = 32$}
  \label{tab:laplace5}
\end{table}

When ran with $B = 1$, $THREADS = 32$, the code takes 318\us/iter $\pm$ 7.8\us/iter.
I do not know what causes this slowdown, but it seems like \texttt{upc\_forall} could contribute to it.

When limiting the modification of \texttt{ex4.upc} to only replace the \texttt{for} loop with a \texttt{upc\_forall} loop, the example becomes $83\%$ slower (206\us/iter $\rightarrow$ 376\us/iter, 8 threads, AMD Ryzen 2700X, \texttt{upcc v2.28.2 + gcc v9.3.0}).

\subsubsection{Note on the update/copy loop}

So far, the program has been running many iterations of the "main" loop, which calls \texttt{iteration()} and \texttt{copy\_array()}.
A \texttt{upc\_barrier;} call is necessary at the end of each of those two operations, as \texttt{copy\_array()} modifies $x$ and depends on the processed value of $x_{new}$, while \texttt{iteration()} modifies $x_{new}$ and depends on the copied value in $x$.

\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// ex4.upc:
shared double x_new[LEN];
shared double x[LEN];
shared double b[LEN];
    \end{lstlisting}
    \caption{Previous array declarations}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// settings.h:
#define BLOCKSIZE 32

// ex5.upc:
shared[BLOCKSIZE] double x_new[LEN];
shared[BLOCKSIZE] double x[LEN];
shared[BLOCKSIZE] double b[LEN];
    \end{lstlisting}
    \caption{New array declarations with blocking factor}
  \end{subfigure}
  \caption{Modification of the array declarations to increase the blocking factor $B$.}
  \label{fig:laplace45a}
\end{figure}

\begin{lstlisting}[style=C, numbers=none, caption={New \texttt{iteration()} and \texttt{copy\_array()} implementations}, label={fig:laplace45b}]
// ex5.upc:
void iteration() {
    upc_forall (size_t i = 1; i < LEN - 1; i++; &x_new[i]) {
        x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
    }

    if (MYTHREAD == 0) x_new[0] = x[0];
    if (MYTHREAD == THREADS - 1) x_new[LEN - 1] = x[LEN - 1];

    upc_barrier;
}

void copy_array() {
    upc_forall (size_t i = 0; i < LEN; i++; &x_new[i]) {
        x[i] = x_new[i];
    }
    upc_barrier;
}
\end{lstlisting}

\newpage

\subsection{Detecting convergence}

One of my last modifications is to measure $\delta_{max} = \max_{i\in \ldbrack 0; n-1 \rdbrack}{\bigl\vert x[i]-x_{new}[i] \bigr\vert}$ (named \texttt{diffmax} in the source code) and to stop once $\delta_{max} \leq \varepsilon$.

Because of the shared nature of $x$ and $x_{new}$, we need to compute a partial $\delta_{max}^t$ and compute $\delta_{max} = \max_{t \in [0; T[}(\delta_{max}^t)$, with:

$$\delta_{max}^t = \max_{i\in \ldbrack 0; n-1 \rdbrack, \; \textrm{\tiny affinity}(x_{new}[i]) = t}{\Bigl\vert x[i]-x_{new}[i] \Bigr\vert}$$

The new timings are:

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 1400.4\us/iter & $\pm$ 6.0\us/iter \\
    4 & 1092.8\us/iter & $\pm$ 17.0\us/iter \\
    8 & 803.4\us/iter & $\pm$ 7.7\us/iter \\
    16 & 647.8\us/iter & $\pm$ 18.9\us/iter \\
    32 & 572.0\us/iter & $\pm$ 4.6\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the fourth UPC implementation of the 1D Laplace solver, $B = 32$, \texttt{MAX\_ITER = 1000}}
  \label{tab:laplace6}
\end{table}

\begin{lstlisting}[style=C, caption={Excerpt from ex6.upc: new \texttt{handle\_diff} function and calculation of \texttt{diffmax}}, label={lst:laplace6}]
// ex6.upc (partial):
shared double diff[THREADS];
shared double diffmax;

void handle_diff(double d) {
    if (d < 0.0) d = -d;
    if (diff[MYTHREAD] < d) diff[MYTHREAD] = d;
}

int main() {
    // ...
    while (true) {
        iteration();

        upc_barrier;

        if (MYTHREAD == 0) {
            for (size_t n = 0; n < THREADS; n++) {
                if (diffmax < diff[n]) diffmax = diff[n];
            }
        }

        upc_barrier;

        if (diffmax <= EPSILON) break;
        if (++iter > MAX_ITER) break;

        copy_array(); // upc_barrier; at the end of copy_array();
    }
    // ...
}

void iteration() {
    upc_forall (size_t i = 1; i < LEN - 1; i++; &x_new[i]) {
        x_new[i] = 0.5 * (x[i-1] + x[i+1] + b[i]);
        handle_diff(x_new[i] - x[i]);
    }
}
\end{lstlisting}

\subsubsection{Using reduction operations}

We can further optimize this code by using reduction operations from \texttt{upc\_collective.h}.
The optimized variant is available at \texttt{\href{https://github.com/adri326/pc40-upc/tree/main/laplace/ex5.upc}{laplace/optimized.upc}}.

By removing the calls to \texttt{upc\_forall}, I could reduce the time taken by a further $35\% (\pm 5\%pt)$.

% Unfortunately, no real performance benefit was observed, even with 32 threads.

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 1216.0\us/iter & $\pm$ 27.1\us/iter \\
    4 & 778.0\us/iter & $\pm$ 15.1\us/iter \\
    8 & 415.8\us/iter & $\pm$ 10\us/iter \\
    16 & 225.8\us/iter & $\pm$ 4.8\us/iter \\
    32 & 130.0\us/iter & $\pm$ 3.1\us/iter \\
    64 & 98.2\us/iter & $\pm$ 2.7\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for the fully optimized UPC implementation of the 1D Laplace solver, $B = 32$, \texttt{MAX\_ITER = 1000}}
  \label{tab:laplace6opt}
\end{table}

\subsection{Conclusion (1D Laplace Solver)}

The first implementation in UPC of the 1D Laplace solver algorithm and its subsequent first optimization (flattening the \texttt{for (...) if (...)}) gave very promising results for the improvement of the speed of the program, bringing the timings down from 140\us/iteration to only 46\us/iteration.

Unfortunately, the next optimization attempt, which introduces a blocking factor to the primary arrays and used \texttt{upc\_forall}, brought the speeds down and made the UPC implementation much slower than the C implementation.

The addition of $\delta_{max}$, as a way to stop the algorithm once a sufficiently accurate solution is found, inevitably added another overhead to the program.
With my best efforts, I could only bring its performance to somewhere between those of the first implementation and its first optimization.

Nonetheless, the resulting program runs slightly faster than the original C implementation, given enough threads).

\section{2D Heat conduction}
\label{sec:heat}

For this second algorithm, I will try to improve on the knowledge gathered from the first algorithm and implement a simple, 2D heat conduction simulation.
As with Section~\ref{sec:laplace}, I begin with a simple C implementation of the algorithm, which will work as my performance base value.
The provided template came bundled with a performance measurement method, but I swapped it out with \texttt{clock()} in the subsequent UPC implementations for consistency with the last section's code.

\subsection{First C implementation}

Following are the performance results for the first C implementation, ran on \texttt{mesoshared} with \texttt{gcc v4.9.0} and on an \texttt{AMD Ryzen 2700X} with \texttt{clang v12.0.1}:

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    \texttt{-O0} & 89.1\us/iter & $\pm$ 0.3\us/iter \\
    \texttt{-O3} & 15.0\us/iter & $\pm$ 0.1\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_c.c} (mesoshared, Listing~\ref{lst:heatc})}
  \label{tab:heatc}
\end{table}

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    \texttt{-O0} & 80.9\us/iter & $\pm$ 2.6\us/iter \\
    \texttt{-O3} & 17.9\us/iter & $\pm$ 2.2\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_c.c} (Ryzen 2700X, Listing~\ref{lst:heatc})}
  \label{tab:heatcr}
\end{table}

\lstinputlisting[style=C, label={lst:heatc}, caption={
  C implementation of the 2D Heat simulation
}]{2d_heat/heat_c.c}

\newpage

\subsection{Porting the C code to UPC}

Porting the template C version to UPC is straightforward.
The performance, however, takes a hit, as the default work sharing causes a lot of remote accesses to memory, despite the blocking factor that was put in place.
We obtain the following measurements on \texttt{mesoshared}:

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 259.9\us/iter & $\pm$ 1.9\us/iter \\
    4 & 193.6\us/iter & $\pm$ 1.5\us/iter \\
    8 & 164.1\us/iter & $\pm$ 1.2\us/iter \\
    16 & 150.6\us/iter & $\pm$ 1.4\us/iter \\
    32 & 151.7\us/iter & $\pm$ 1.7\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_1.upc}}
  \label{tab:heat1}
\end{table}

\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// heat_c.c:
for (i=1; i<N+1; i++) {
  for (j=1; j<N+1; j++) {
    T = 0.25 *
      (grid[i+1][j] + grid[i-1][j] +
        grid[i][j-1] + grid[i][j+1]); /* stencil */
    dT = T - grid[i][j]; /* local variation */
    new_grid[i][j] = T;
    if (dTmax < fabs(dT)) dTmax = fabs(dT); /* max variation in this iteration */
  }
}
    \end{lstlisting}
    \caption{Main loop in \texttt{heat\_c.c}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\columnwidth}
    \begin{lstlisting}[style=C, numbers=none]
// heat_1.upc:
for (size_t i = 1; i <= N; i++) {
  upc_forall (size_t j = 1; j <= N; j++; &grid[i][j]) {
    double T = 0.25 * (grid[i+1][j] + grid[i-1][j] + grid[i][j-1] + grid[i][j+1]);
    double dT = fabs(T - grid[i][j]);
    new_grid[i][j] = T;
    if (dTmax < dT) dTmax = dT;
  }
}
    \end{lstlisting}
    \caption{Main loop in \texttt{heat\_1.upc}}
  \end{subfigure}
  \caption{Porting the main loop from C to UPC}
  \label{fig:heatc1}
\end{figure}

\newpage

\subsection{Optimizing the array accesses}
\label{sec:ptrswap}

One simple optimization is to avoid copying the destination array (\texttt{new\_grid}) into the source array (\texttt{grid}).

To do this, I store two shared pointers that will point on either array, and I swap them at the end of each iteration, simulating a copy of \texttt{grid} into \texttt{new\_grid}, with a $\mathcal{O}(1)$ time complexity instead of $\mathcal{O}(n)$.
Doing so effectively halves the amount of synchronization that needs to happen for each loop, which can be observed in the timing results:

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 134.0\us/iter & $\pm$ 1.4\us/iter \\
    4 & 103.9\us/iter & $\pm$ 4.1\us/iter \\
    8 & 87.4\us/iter & $\pm$ 1.0\us/iter \\
    16 & 81.4\us/iter & $\pm$ 1.2\us/iter \\
    32 & 82.9\us/iter & $\pm$ 1.5\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_3.upc}}
  \label{tab:heat3}
\end{table}

This optimization was also done on the C version for comparison, and I obtain the following speeds:

\begin{table}[h]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    \texttt{-O0} & 64.0\us/iter & $\pm$ 0.5\us/iter \\
    \texttt{-O3} & 11.3\us/iter & $\pm$ 0.4\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_c\_ptr.c}}
  \label{tab:heatcptr}
\end{table}

\begin{lstlisting}[style=C, numbers=none, caption={Excerpt from \texttt{heat\_3.upc}, where pointer-swapping was implemented}, label={lst:heat3}]
// heat_3.upc:
shared[BLOCKSIZE] double (*ptr)[N+2] = grid;
shared[BLOCKSIZE] double (*new_ptr)[N+2] = new_grid;

// ...

if (n_iter % 2 == 0) {
  ptr = grid;
  new_ptr = new_grid;
} else {
  ptr = new_grid;
  new_ptr = grid;
}

// ...

double T = 0.25 * (ptr[i+1][j] + ptr[i-1][j] + ptr[i][j-1] + ptr[i][j+1]);
double dT = fabs(T - ptr[i][j]);
new_ptr[i][j] = T;
\end{lstlisting}

\subsection{Array privatization}
\label{sec:heat4}

As observed in the later examples of the 1D Laplace equation solver (Table~\ref{tab:laplace5}), \texttt{upc\_forall} has a significant performance cost over a regular \texttt{for} loop.
We also know that local shared memory accesses have a considerable cost when the shared pointer math is done without dedicated hardware support \cite{OlivierSerres:2015}.

To make use of less shared accesses, the program can copy a chunk of the $grid$ array into private memory using \texttt{upc\_memget} (into \texttt{ptr\_priv} and \texttt{new\_ptr\_priv}).
We specifically only retrieve the memory that has the affinity to the current thread, so that the code can write to it and broadcast it in one go.
The memory is copied also duplicated to make use of pointer-swapping as in Section~\ref{sec:ptrswap}.

We then operate on the private array, only using remote accesses when necessary, and store the results in the new private array.
Once the work is finished, the program put the new private array into \texttt{new\_grid} with \texttt{upc\_memput} and synchronize $\delta_{max}$.

\begin{table}[ht]
  \centering\begin{tabular}{|c|c|c|}
    \hline
    Threads & Time (avg) & CI ($\sigma=0.01$) \\
    \hline
    2 & 13.5\us/iter & $\pm$ 1.4\us/iter \\
    4 & 12.6\us/iter & $\pm$ 1.2\us/iter \\
    8 & 13.3\us/iter & $\pm$ 1.0\us/iter \\
    16 & 15.0\us/iter & $\pm$ 1.2\us/iter \\
    32 & 21.5\us/iter & $\pm$ 1.1\us/iter \\
    \hline
  \end{tabular}
  \caption{Timing results for \texttt{heat\_4.upc} (Listing~\ref{lst:heat4})}
  \label{tab:heat4}
\end{table}

\lstinputlisting[style=C, label={lst:heat4}, caption={
  Optimized UPC implementation of the 2D Heat algorithm (\texttt{heat\_4.upc})
}]{2d_heat/heat_4.upc}

\subsection{Dynamic problem size}

This last version of the code is not an optimization: we had to adapt it so that we could specify the number of threads and the size of the grid at runtime, rather than at compile time.
This requires changing a big portion of the code, as it was previously using pre-defined constants for these two parameters.

Because speed for this part is not a worry, I based the code from \texttt{heat\_3.upc}, as the previous optimization step made the code harder to work with.

The declaration of \texttt{grid} and \texttt{new\_grid} have been deleted and are done implicitely through the compiler-supplied function \texttt{upc\_all\_alloc}.
The matrix notation, while useful, had to be gotten rid of, as it was dependent on $N$, which is now dynamic.
This meant re-writing all of the matrix accesses to now multiply the $y$ coordinate by $(n+2)$ and to add its result to the $x$ coordinate.

\lstinputlisting[style=C, label={lst:heat5}, caption={
  Dynamic UPC implementation of the 2D Heat algorithm (\texttt{heat\_5.upc})
}]{2d_heat/heat_5.upc}

\section{Conclusion}

For both the Simplified 1D Laplace Equation Solver (Section~\ref{sec:laplace}) and the 2D Heat Algorithm (Section~\ref{sec:heat}), I was able to take a simple C implementation and port it to UPC with minimal effort.
These two problems are easily parallelizable, as all of the work within an iteration can be done independently (or vertically).

We then went along the process of refining the UPC implementation and optimizing the various accesses to the arrays to reduce the overhead of parallelization induced by UPC.
Unfortunately, the overhead encountered here is mainly caused by the shared pointer operations (as highlighted by the results in Section~\ref{sec:laplace5} and Section~\ref{sec:heat4}).
Minimizing this overhead required heavy modification to the code.

With all of my efforts, the UPC implementation only came shortly before the C implementation when benchmarked and required at least 8 threads to overcome the language's overhead.
This difficulty to beat the speed of C is explained by the nature of the problems at hand: each iteration consists of a small amount of work over a large set of data, that needs to be synchronized before the next iteration.
Even with a perfect parallelization of the algorithm (with no synchronization overhead), the implementation would still be bottlenecked by the data transfer speeds and the slowest thread contributing to the calculation.

\subsection{Speed comparison}

Following are two comparative tables of all of the measurements of this report.

The Simplified 1D Laplace Equation Solver scored great until I introduced $\delta_{max}$, which required some costly synchronization.

The 2D Heat Algorithm did worse, only achieving speeds close to those of C.
The synchronization costs made it unfeasible to use a greater amount of threads, as can be seen with the last few rows.

\begin{table}[!htp]
  \centering\begin{tabular}{|c|l|r|l|}
    \hline
    Implementation & Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline \hline
    \multirow{2}{*}{\hyperref[tab:laplace2]{\texttt{laplace/ex2.c}}}
    & \texttt{-O0} & 450\us/iter & $\pm$ 10\us/iter \\
    & \texttt{-O3} & 140\us/iter & $\pm$ 10\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:laplace3]{\texttt{laplace/ex3.upc}} \\
      (w/o $\delta_{max}$)
    }}
    & $T=2$ & 515.2\us/iter & $\pm$ 3.9\us/iter \\
    & $T=4$ & 312.4\us/iter & $\pm$ 2.0\us/iter \\
    & $T=8$ & 204.0\us/iter & $\pm$ 2.0\us/iter \\
    & $T=16$ & 166.2\us/iter & $\pm$ 2.5\us/iter \\
    & $T=32$ & 150.4\us/iter & $\pm$ 3.2\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:laplace4]{\texttt{laplace/ex4.upc}} \\
      (w/o $\delta_{max}$)
    }}
    & $T=2$ & 482.0\us/iter & $\pm$ 3.2\us/iter \\
    & $T=4$ & 265.2\us/iter & $\pm$ 2.3\us/iter \\
    & $T=8$ & 128.8\us/iter & $\pm$ 1.6\us/iter \\
    & $T=16$ & 74.6\us/iter & $\pm$ 3.1\us/iter \\
    & $T=32$ & 46.0\us/iter & $\pm$ 2.5\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:laplace5]{\texttt{laplace/ex5.upc}} \\
      (with $\delta_{max}$)
    }}
    & $T=2$ & 959.2\us/iter & $\pm$ 3.9\us/iter \\
    & $T=4$ & 863.9\us/iter & $\pm$ 16.1\us/iter \\
    & $T=8$ & 691.6\us/iter & $\pm$ 6.1\us/iter \\
    & $T=16$ & 581.5\us/iter & $\pm$ 4.1\us/iter \\
    & $T=32$ & 536.9\us/iter & $\pm$ 4.0\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:laplace6]{\texttt{laplace/ex6.upc}} \\
      (with $\delta_{max}$)
    }}
    & $T=2$ & 1400.4\us/iter & $\pm$ 6.0\us/iter \\
    & $T=4$ & 1092.8\us/iter & $\pm$ 17.0\us/iter \\
    & $T=8$ & 803.4\us/iter & $\pm$ 7.7\us/iter \\
    & $T=16$ & 647.8\us/iter & $\pm$ 18.9\us/iter \\
    & $T=32$ & 572.0\us/iter & $\pm$ 4.6\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:laplace6opt]{\texttt{laplace/optimized.upc}} \\
      (with $\delta_{max}$)
    }}
    & $T=2$ & 1216.0\us/iter & $\pm$ 27.1\us/iter \\
    & $T=4$ & 778.0\us/iter & $\pm$ 15.1\us/iter \\
    & $T=8$ & 415.8\us/iter & $\pm$ 10\us/iter \\
    & $T=16$ & 225.8\us/iter & $\pm$ 4.8\us/iter \\
    & $T=32$ & 130.0\us/iter & $\pm$ 3.1\us/iter \\
    & $T=64$ & 98.2\us/iter & $\pm$ 2.7\us/iter \\
    \hline
  \end{tabular}
  \caption{Comparative table of the benchmarks of the Simplified Laplace 1D Solver}
  \label{tab:laplace}
\end{table}

\begin{table}[!htp]
  \centering\begin{tabular}{|c|l|r|l|}
    \hline
    Implementation & Options & Time (avg) & CI ($\sigma=0.01$) \\
    \hline \hline
    \multirow{2}{*}{\makecell{
      \hyperref[tab:heatc]{\texttt{heat\_c.c}} \\
      (w/o ptr-swap)
    }}
    & \texttt{-O0} & 80.9\us/iter & $\pm$ 2.6\us/iter \\
    & \texttt{-O3} & 17.9\us/iter & $\pm$ 2.2\us/iter \\
    \hline
    \multirow{2}{*}{\makecell{
      \hyperref[tab:heatcptr]{\texttt{heat\_c\_ptr.c}} \\
      (with ptr-swap)
    }}
    & \texttt{-O0} & 64.0\us/iter & $\pm$ 0.5\us/iter \\
    & \texttt{-O3} & 11.3\us/iter & $\pm$ 0.4\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:heat1]{\texttt{heat\_1.upc}} \\
      (w/o ptr-swap)
    }}
    & $T=2$ & 259.9\us/iter & $\pm$ 1.9\us/iter \\
    & $T=4$ & 193.6\us/iter & $\pm$ 1.5\us/iter \\
    & $T=8$ & 164.1\us/iter & $\pm$ 1.2\us/iter \\
    & $T=16$ & 150.6\us/iter & $\pm$ 1.4\us/iter \\
    & $T=32$ & 151.7\us/iter & $\pm$ 1.7\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:heat3]{\texttt{heat\_3.upc}} \\
      (with ptr-swap)
    }}
    & $T=2$ & 134.0\us/iter & $\pm$ 1.4\us/iter \\
    & $T=4$ & 103.9\us/iter & $\pm$ 4.1\us/iter \\
    & $T=8$ & 87.4\us/iter & $\pm$ 1.0\us/iter \\
    & $T=16$ & 81.4\us/iter & $\pm$ 1.2\us/iter \\
    & $T=32$ & 82.9\us/iter & $\pm$ 1.5\us/iter \\
    \hline
    \multirow{5}{*}{\makecell{
      \hyperref[tab:heat4]{\texttt{heat\_4.upc}} \\
      (with ptr-swap)
    }}
    & $T=2$ & 13.5\us/iter & $\pm$ 1.4\us/iter \\
    & $T=4$ & 12.6\us/iter & $\pm$ 1.2\us/iter \\
    & $T=8$ & 13.3\us/iter & $\pm$ 1.0\us/iter \\
    & $T=16$ & 15.0\us/iter & $\pm$ 1.2\us/iter \\
    & $T=32$ & 21.5\us/iter & $\pm$ 1.1\us/iter \\
    \hline
  \end{tabular}
  \caption{Comparative table of the benchmarks of the 2D Heat Algorithm}
  \label{tab:heat}
\end{table}

\subsection{Personal remarks}

Overall, I enjoyed working with UPC and optimizing the parallelized code.
However, I feel like UPC is lacking behind in terms of what the compiler can optimize for the developer.
For instance, it could unwrap a \texttt{upc\_forall} loop into two \texttt{for} loops when the increment is a divisor of the blocking factor: this can easily be done with macros already and can only make the code faster.

The hardest part of working on this project was finding documentation for the language itself.
The documentation is scattered across internet, with some tutorials explaining how basic functions work but missing the more complicated functions, some documentation hosted on code mirroring websites and the rest burried inside of the compiler's source code.

Searching for example \textit{"upc\_all\_reduceD"} online brings up \href{https://upc.lbl.gov/hypermail/upc-users/0550.html}{an 11-year old mail archive}, \href{https://github.com/ROCm-Developer-Tools/OSU_Microbenchmarks/blob/master/upc/osu_upc_all_broadcast.c}{a benchmark of UPC} and quite further down the summary of the UPC Collective specification.
The only way to truly know what this function does is to \href{https://fossies.org/linux/berkeley_upc/upc-tests/UPC-Coll-RefImp/upc_all_reduce.c}{read its source code}.

This contrasts with other, smaller languages that I have encountered until now, whose documentation is usually centered in one place.
With the similarly niche language "Pony", searching \textit{"ponylang hashmap"} online yields \href{https://stdlib.ponylang.io/collections-HashMap/}{a rendered version of the documentation of the language's standard library} as the first result.

The errors outputted by the language were also of little help, as they did not show any additional information. The \texttt{--verbose} flag only displayed information unrelated to the error.

\begin{figure}
  \begin{lstlisting}[style=C]
$ upcc 2d_heat/heat_5.upc -o build/heat_5
upcc: error during UPC-to-C translation (sgiupc stage):
2d_heat/heat_5.upc: In function 'main':
2d_heat/heat_5.upc:66: incompatible type for argument 1 of 'bupc_all_reduceD'
  \end{lstlisting}
  \caption{\textit{"I guess the compiler wants me to learn the function signature by myself."}}
\end{figure}

Nonetheless, I am glad that I got to try out this language and work with parallelized code.

\newpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{report}

\end{document}
